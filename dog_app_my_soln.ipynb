{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with transfer learning from InceptionV3 trained on Image net\n",
    "### This is the implentation of step 6 and 7 in the dog_app notebook.\n",
    "\n",
    "This is here so I can train and get the best CNN for classifying dog breeds to put into an app down the road..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.datasets import load_files  \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human and dog detector functions\n",
    "\n",
    "Human detector uses opencv, dog detector uses pretrained ResNet50 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image as image_processor                 \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image_processor.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image_processor.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))\n",
    "\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bottleneck features from the train dog pictures ran on the inception V3 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogInceptionV3Data.npz')\n",
    "train_InceptionV3 = bottleneck_features['train']\n",
    "valid_InceptionV3 = bottleneck_features['valid']\n",
    "test_InceptionV3 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dog labels from dataset, don't need features since we are using the features listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "_, train_targets = load_dataset('dogImages/train')\n",
    "_, valid_targets = load_dataset('dogImages/valid')\n",
    "_, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              6294528   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 7,936,133\n",
      "Trainable params: 7,936,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "inception_model = Sequential()\n",
    "\n",
    "inception_model.add(GlobalAveragePooling2D(input_shape=train_InceptionV3.shape[1:]))\n",
    "\n",
    "#overfits heavily, two layers are good before output, better accuracy by increasing number of nodes for first fc\n",
    "inception_model.add(Dense(3072, activation='relu'))\n",
    "inception_model.add(Dropout(0.5))\n",
    "inception_model.add(Dense(512, activation='relu'))#leave this 512, any bigger causes too much overfitting\n",
    "inception_model.add(Dropout(0.3))\n",
    "inception_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compiliation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers as opt \n",
    "#need low learning rate to help overfitting, takes forever to train tho\n",
    "inception_model.compile(loss='categorical_crossentropy', optimizer=opt.RMSprop(lr=0.000001), \n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4096 - acc: 0.8744 - val_loss: 0.4751 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47511, saving model to saved_models/weights.best.transfer.inception.hdf5\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4000 - acc: 0.8753 - val_loss: 0.4762 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47511\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4081 - acc: 0.8744 - val_loss: 0.4760 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47511\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4150 - acc: 0.8744 - val_loss: 0.4758 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47511\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4055 - acc: 0.8757 - val_loss: 0.4764 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47511\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.3940 - acc: 0.8783 - val_loss: 0.4774 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.47511\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4093 - acc: 0.8757 - val_loss: 0.4774 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.47511\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.3972 - acc: 0.8711 - val_loss: 0.4781 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.47511\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.4082 - acc: 0.8726 - val_loss: 0.4775 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.47511\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 12s 2ms/step - loss: 0.3912 - acc: 0.8820 - val_loss: 0.4793 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.47511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b4dd80e80>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "#from keras.preprocessing.image import ImageDataGenerator #for data augmentation\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.transfer.inception.test.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "                           \n",
    "inception_model.fit(train_InceptionV3, train_targets, validation_data=(valid_InceptionV3, valid_targets),\n",
    "          epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best validation loss weights:\n",
    "Pretty much the best I am going to get without some super huge networks and large training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inception_model.load_weights('saved_models/weights.best.transfer.inception.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 83.13\n"
     ]
    }
   ],
   "source": [
    "inception_predictions = [np.argmax(inception_model.predict(\n",
    "                        np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\n",
    "print('Test accuracy: {0:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting dog breeds using trained model functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input as pre_process_input\n",
    "inception_transferred = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "#extract_bottleneck_features.py recreated the net everytime function was called, causing huge slowdowns and higher\n",
    "#memory usage with each call, should only be ran once to create the object\n",
    "def extract_InceptionV3_2(tensor):\n",
    "    return inception_transferred.predict(pre_process_input(tensor))\n",
    "\n",
    "def inception_predict_breed(image_path):\n",
    "    botneck_feature = extract_InceptionV3_2(path_to_tensor(image_path))\n",
    "    predicted_breed = np.argmax(inception_model.predict(botneck_feature))\n",
    "    \n",
    "    return dog_names[predicted_breed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier using trained model and dog/human detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "def classifer(image_path):\n",
    "    display_img = np.array(mpimg.imread(image_path), dtype=float)\n",
    "    if face_detector(image_path) and not dog_detector(image_path):\n",
    "        print(\"Hello human!\")\n",
    "        plt.imshow(display_img)\n",
    "        plt.show()\n",
    "        print(\"You look like a {} dog!!\".format(inception_predict_breed(image_path)))\n",
    "    elif dog_detector(image_path) and not face_detector(image_path):\n",
    "        print(\"Hello dog!\")\n",
    "        plt.imshow(display_img)\n",
    "        plt.show()\n",
    "        print(\"You are a {} dog!! {}/10\".format(inception_predict_breed(image_path), np.random.randint(\n",
    "                                                                                     low=10, high=15, size=None)))\n",
    "    else:\n",
    "        print(\"Hello thing!\")\n",
    "        plt.imshow(display_img)\n",
    "        plt.show()\n",
    "        print(\"I don't know what you are, please try another picture!\")\n",
    "        print(\"If I wrong, then you may be a {}\".format(inception_predict_breed(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all images in images/step7_test_images and run the classifier on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_images = np.array(glob('images/step7_test_images/*'))\n",
    "np.random.shuffle(sample_images)\n",
    "\n",
    "for sample_image in sample_images:\n",
    "    classifer(sample_image)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
